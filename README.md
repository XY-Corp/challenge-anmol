# challenge-anmol

Below is the complete Contestant Manual for the ‚ÄúSarcasm-in-the-Browser‚Äù fine-tuning challenge. It explains the goal, resources, step-by-step workflow, deliverables, and evaluation rubric. Feel free to copy it verbatim into your onboarding docs or tweak the tone to match your brand voice.

‚∏ª

Executive summary

You will fine-tune a compact DistilBERT encoder to recognise sarcasm, export the model to an 8-bit ONNX file, and run it entirely client-side in a single-page React/Vite app. Public sarcasm datasets (~10 k sentences) fit on a laptop, and INT8 quantisation shrinks the model to ‚âà 18 MB, achieving sub-200 ms latency with WebGPU on mainstream hardware. The assignment tests your skill across data curation, Hugging Face Trainer, optimisation with Optimum, and front-end deployment using either Transformers.js or ONNX Runtime Web. Ôøº Ôøº Ôøº Ôøº

‚∏ª

1‚ÄÇChallenge overview

1.1‚ÄÇObjective

Create a browser-only sarcasm detector that takes an English sentence and returns a confidence score (0 = sincere, 1 = sarcastic) plus a friendly emoji. The entire pipeline‚Äîtokeniser, model, post-processing‚Äîmust run locally in the user‚Äôs tab; no server calls allowed. Ôøº Ôøº

1.2‚ÄÇWhy sarcasm?

Binary labels keep the task approachable, yet sarcasm demands nuanced language understanding, so rule-based baselines fall short. This lets us judge your modelling chops without huge compute. Ôøº Ôøº

1.3‚ÄÇRecommended ingredients

Component	Default pick	Rationale	Key spec
Base model	distilbert-base-uncased	66 M params, 6 layers, 97 % Bert-style accuracy at 40 % size	~250 MB FP32 ‚Üí ‚â§20 MB INT8
Datasets	Sarcasm Corpus V2, iSarcasm, News-Headlines	Balanced labels, permissive licences	~9 k ‚Äì 20 k samples
Exporter	Optimum CLI export onnx --quantize dynamic	One-command ONNX + quantisation	18‚Äì22 MB ONNX
Browser runtime	(A) Transformers.js pipeline ‚Üí WASM/WebGPU, or (B) ONNX Runtime Web with WebGPU EP	Both support INT8; WebGPU halves latency on modern laptops	<150 ms on desktop
Frontend	React + Vite	Instant HMR dev-server, zero-config static build	Bundle ‚â§1 MB gz



‚∏ª

2‚ÄÇDeliverables
	1.	Public Git repo with reproducible code and a README.md detailing:
	‚Ä¢	dataset(s) used and licence links
	‚Ä¢	training hyper-parameters
	‚Ä¢	final metrics (F1, accuracy)
	2.	model.onnx ‚â§ 25 MB and tokenizer.json committed under /public/model.
	3.	Browser demo at <your-url>/sarcasm/ with:
	‚Ä¢	textarea input
	‚Ä¢	sarcasm probability bar ¬± emoji
	‚Ä¢	latency log in console
	4.	Short screencast (<2 min, GIF or MP4) or live URL proving offline mode‚Äîreload with Wi-Fi disabled.

‚∏ª

3‚ÄÇTimeline & workload

Phase	Suggested effort	Deadline
Kick-off & environment setup	¬Ω day	Day 1
Data wrangling & EDA	¬Ω day	Day 2
Fine-tune & evaluate	1 day	Day 3
Quantise & export	¬Ω day	Day 4
Front-end integration & polish	1 day	Day 5
Buffer, screencast & submit	¬Ω day	Day 6



‚∏ª

4‚ÄÇStep-by-step guide

4.1‚ÄÇEnvironment

conda create -n sarcasm python=3.10
conda activate sarcasm
pip install "transformers>=4.40" datasets evaluate accelerate \
           "optimum[onnxruntime,gpu]" onnxruntime-web==1.17.0 \
           @huggingface/transformers        # for JS side

ONNX Runtime Web ‚â• 1.17 adds official WebGPU support. Ôøº

4.2‚ÄÇLoad data

from datasets import load_dataset
ds = load_dataset("Orbay/sarcasm_corpus_v2", "default")  # or your choice

Filter/clean as needed; keep a 10 % hold-out test split. Ôøº

4.3‚ÄÇFine-tune with Trainer

Follow the Hugging Face sequence-classification recipe; two epochs, learning-rate 2 e-5, batch 16 on a single GPU hits F1 ‚âà 0.82. Ôøº Ôøº

4.4‚ÄÇCheckpoint & metrics

Save the best checkpoint (save_total_limit=1) and log F1/accuracy with evaluate for reproducibility. Ôøº

4.5‚ÄÇExport & quantise

optimum-cli export onnx --model path/to/ckpt onnx/ --quantize dynamic

Dynamic INT8 squeezes DistilBERT to ‚âà 18 MB with negligible accuracy loss. Ôøº

4.6‚ÄÇBrowser deployment

Option A ‚Äî Transformers.js

import { pipeline } from '@huggingface/transformers';
const clf = await pipeline('text-classification', '/public/model', { quantized: true });
const { label, score } = (await clf(userInput))[0];

Transforms.js auto-detects WebGPU when enabled. Ôøº Ôøº

Option B ‚Äî ONNX Runtime Web

import * as ort from 'onnxruntime-web';
const sess = await ort.InferenceSession.create('/model.onnx',
           { executionProviders: ['webgpu', 'wasm'] });

WebGPU cuts inference nearly in half vs pure WASM. Ôøº

4.7‚ÄÇUI polish
	‚Ä¢	Progress bar coloured by score (score > 0.5 ? ü´† : üôÇ).
	‚Ä¢	Optional token heat-map via attention weights if you fancy.
	‚Ä¢	Build static site:

npm run build             # Vite

and deploy to GitHub Pages or Vercel. Ôøº Ôøº

‚∏ª

5‚ÄÇEvaluation rubric

Category	Threshold (pass)	Bonus
Accuracy	F1 ‚â• 0.80	F1 > 0.85 or multilingual
Model size	‚â§ 25 MB	‚â§ 10 MB (4-bit / pruning)
Latency	‚â§ 150 ms on desktop	‚â§ 200 ms on mid-tier phone
UX	clear score + emoji	token heat-map, PWA offline
Code	reproducible scripts	CI pipeline, Dockerfile



‚∏ª

6‚ÄÇSubmission checklist
	‚Ä¢	Repo pushed, public or add us as collaborators
	‚Ä¢	README.md explains data licences & commands
	‚Ä¢	model.onnx + tokenizer.json in /public/model
	‚Ä¢	npm run build artefacts committed (or live url)
	‚Ä¢	Screencast or hosted demo link provided

‚∏ª

7‚ÄÇFurther reading
	‚Ä¢	Sarcasm Corpus V2 dataset card‚ÄÇ Ôøº
	‚Ä¢	iSarcasm paper & dataset‚ÄÇ Ôøº
	‚Ä¢	DistilBERT model card‚ÄÇ Ôøº
	‚Ä¢	Optimum ONNX export guide‚ÄÇ Ôøº
	‚Ä¢	ONNX Runtime WebGPU announcement‚ÄÇ Ôøº
	‚Ä¢	Quantisation docs (8-bit)‚ÄÇ Ôøº
	‚Ä¢	Transformers.js pipeline docs‚ÄÇ Ôøº
	‚Ä¢	HF Trainer sequence-classification tutorial‚ÄÇ Ôøº
	‚Ä¢	Vite static deployment guide‚ÄÇ Ôøº
	‚Ä¢	Vercel deployment overview‚ÄÇ Ôøº

Good luck‚Äîshow us your wit-detecting wizardry!
