
# Sarcasmâ€‘inâ€‘theâ€‘Browser Fineâ€‘Tuning Challenge  
**Contestant Manual**

---

## Executive Summary
You will fineâ€‘tune a compact DistilBERT encoder to recognise sarcasm, export the model to an 8â€‘bit ONNX file and run it entirely clientâ€‘side in a singleâ€‘page React/Vite app. Public sarcasm datasets (~10â€¯k sentences) fit on a laptop, and INT8 quantisation shrinks the model to â‰ˆâ€¯18â€¯MB, achieving subâ€‘200â€¯ms latency with WebGPU on mainstream hardware. The assignment tests your skill across data curation, Hugging Face Trainer, optimisation with Optimum, and frontâ€‘end deployment using either Transformers.js or ONNX RuntimeÂ Web.

---

## 1â€‚Challenge Overview

### 1.1â€‚Objective
Create a browserâ€‘only sarcasm detector that takes an English sentence and returns a confidence score (0Â = sincere, 1Â = sarcastic) plus a friendly emoji. The entire pipelineâ€”tokeniser, model, postâ€‘processingâ€”must run locally in the userâ€™s tab; no server calls allowed.

### 1.2â€‚Why Sarcasm?
Binary labels keep the task approachable, yet sarcasm demands nuanced language understanding, so ruleâ€‘based baselines fall short. This lets us judge your modelling chops without huge compute.

### 1.3â€‚Recommended Ingredients

| Component | Default Pick | Rationale | Key Spec |
|-----------|--------------|-----------|----------|
| **Base model** | `distilbert-base-uncased` | 66â€¯Mâ€¯params, 6â€¯layers, 97â€¯% BERTâ€‘style accuracy at 40â€¯% size | ~250â€¯MB FP32 â†’ â‰¤â€¯20â€¯MB INT8 |
| **Datasets** | Sarcasm CorpusÂ V2, iSarcasm, Newsâ€‘Headlines | Balanced labels, permissive licences | ~9â€¯kâ€“20â€¯k samples |
| **Exporter** | OptimumÂ CLI `export onnx --quantize dynamic` | Oneâ€‘command ONNXÂ +Â quantisation | 18â€“22â€¯MB ONNX |
| **Browser runtime** | (A) Transformers.js pipeline (WASM/WebGPU) **or** (B) ONNXÂ RuntimeÂ Web with WebGPU EP | Both support INT8; WebGPU halves latency on modern laptops | <â€¯150â€¯ms on desktop |
| **Frontend** | ReactÂ +Â Vite | Instant HMR devâ€‘server, zeroâ€‘config static build | Bundle â‰¤â€¯1â€¯MBâ€¯gz |

---

## 2â€‚Deliverables

1. **Public Git repo** with reproducible code and a `README.md` detailing:  
   * dataset(s) used and licence links  
   * training hyperâ€‘parameters  
   * final metrics (F1, accuracy)  
2. **`model.onnx`** â‰¤â€¯25â€¯MB and **`tokenizer.json`** committed under `/public/model`.  
3. **Browser demo** at `<your-url>/sarcasm/` with:  
   * textarea input  
   * sarcasm probability barâ€¯Â±â€¯emoji  
   * latency log in console  
4. **Short screencast** (<â€¯2â€¯min, GIFÂ or MP4) or live URL proving offline modeâ€”reload with Wiâ€‘Fi disabled.  

---

## 3â€‚Timeline & Workload

| Phase | Suggested Effort | Deadline |
|-------|------------------|----------|
| Kickâ€‘off & environment setup | Â½â€¯day | DayÂ 1 |
| Data wrangling & EDA | Â½â€¯day | DayÂ 2 |
| Fineâ€‘tune & evaluate | 1â€¯day | DayÂ 3 |
| Quantise & export | Â½â€¯day | DayÂ 4 |
| Frontâ€‘end integration & polish | 1â€¯day | DayÂ 5 |
| Buffer, screencast & submit | Â½â€¯day | DayÂ 6 |

---

## 4â€‚Stepâ€‘byâ€‘Step Guide

### 4.1â€‚Environment
```bash
conda create -n sarcasm python=3.10
conda activate sarcasm
pip install "transformers>=4.40" datasets evaluate accelerate \
           "optimum[onnxruntime,gpu]" onnxruntime-web==1.17.0 \
           @huggingface/transformers        # for JS side
```
ONNXÂ RuntimeÂ Webâ€¯â‰¥â€¯1.17 adds official WebGPU support.

### 4.2â€‚Load Data
```python
from datasets import load_dataset
ds = load_dataset("Orbay/sarcasm_corpus_v2", "default")  # or your choice
```
Filter/clean as needed; keep a 10â€¯% holdâ€‘out test split.

### 4.3â€‚Fineâ€‘Tune with `Trainer`
Follow the HuggingÂ Face sequenceâ€‘classification recipe; two epochs, learning rateâ€¯2eâ€‘5, batchâ€¯16 on a single GPU hits F1Â â‰ˆâ€¯0.82.

### 4.4â€‚Checkpoint & Metrics
Save the best checkpoint (`save_total_limit=1`) and log F1/accuracy with `evaluate` for reproducibility.

### 4.5â€‚Export & Quantise
```bash
optimum-cli export onnx --model path/to/ckpt onnx/ --quantize dynamic
```
Dynamic INT8 squeezes DistilBERT to â‰ˆâ€¯18â€¯MB with negligible accuracy loss.

### 4.6â€‚Browser Deployment

#### OptionÂ A â€” Transformers.js
```js
import { pipeline } from '@huggingface/transformers';
const clf = await pipeline('text-classification', '/public/model', { quantized: true });
const { label, score } = (await clf(userInput))[0];
```
Transformers.js autoâ€‘detects WebGPU when enabled.

#### OptionÂ B â€” ONNX RuntimeÂ Web
```js
import * as ort from 'onnxruntime-web';
const sess = await ort.InferenceSession.create('/model.onnx', {
  executionProviders: ['webgpu', 'wasm']
});
```
WebGPU cuts inference nearly in half vs pureÂ WASM.

### 4.7â€‚UI Polish
* Progress bar coloured by score (`score > 0.5 ? ðŸ«  : ðŸ™‚`).  
* Optional token heatâ€‘map via attention weights if you fancy.  
* Build static site:  
  ```bash
  npm run build             # Vite
  ```
  then deploy to GitHubÂ Pages or Vercel.

---

## 5â€‚Evaluation Rubric

| Category | Threshold (Pass) | Bonus |
|----------|------------------|-------|
| **Accuracy** | F1â€¯â‰¥â€¯0.80 | F1â€¯>â€¯0.85 or multilingual |
| **Model size** | â‰¤â€¯25â€¯MB | â‰¤â€¯10â€¯MB (4â€‘bit / pruning) |
| **Latency** | â‰¤â€¯150â€¯ms on desktop | â‰¤â€¯200â€¯ms on midâ€‘tier phone |
| **UX** | clear scoreÂ +Â emoji | token heatâ€‘map, PWA offline |
| **Code** | reproducible scripts | CI pipeline, Dockerfile |

---

## 6â€‚Submission Checklist

- [ ] Repo pushed, public or collaborators added  
- [ ] `README.md` explains data licences & commands  
- [ ] `model.onnx` + `tokenizer.json` in `/public/model`  
- [ ] `npm run build` artefacts committed (or live URL)  
- [ ] Screencast or hosted demo link provided  

---

## 7â€‚Further Reading

* Sarcasm CorpusÂ V2 dataset card  
* iSarcasm paper & dataset  
* DistilBERT model card  
* Optimum ONNX export guide  
* ONNXÂ Runtime WebGPU announcement  
* Quantisation docs (8â€‘bit)  
* Transformers.js pipeline docs  
* HF Trainer sequenceâ€‘classification tutorial  
* Vite static deployment guide  
* Vercel deployment overview  

Good luckâ€”show us your witâ€‘detecting wizardry!
